{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MaskRCNN_Tutorial.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"QHnVupBBn9eR"},"source":["# Mask R-CNN Beginner's Tutorial\n"]},{"cell_type":"markdown","metadata":{"id":"vM54r6jlKTII"},"source":["## Install Google drive\n"]},{"cell_type":"code","metadata":{"id":"FsePPpwZSmqt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a4da6b07-39b9-4f4b-f618-ec17a435f291","executionInfo":{"status":"ok","timestamp":1655583028381,"user_tz":-480,"elapsed":17651,"user":{"displayName":"林永隆","userId":"15129329272009530424"}}},"source":["from google.colab import drive\n","import sys, datetime\n","\n","# Connect Google drive with authentication\n","drive.mount(\"/content/gdrive\", force_remount=True)\n","\n","# Set root dir of the project\n","ROOT_DIR = \"/content/gdrive/MyDrive/MaskRCNN_Tutorial/\"\n","sys.path.insert(0, ROOT_DIR)\n","# List file information in the path\n","%ls $ROOT_DIR\n","\n","# Set training phase with time stamp\n","train_phase = \"OralCancer_{:%Y%m%dT%H%M}\".format(datetime.datetime.now())\n","print(\"\\nSet training phase: {}\".format(train_phase))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","\u001b[0m\u001b[01;34mDATASET\u001b[0m/  dataset_split.py  MaskRCNN_Tutorial.ipynb  \u001b[01;34mmrcnn\u001b[0m/  README.md\n","\n","Set training phase: OralCancer_20220618T2010\n"]}]},{"cell_type":"markdown","source":["## Import required library"],"metadata":{"id":"kK_-RBawT9HO"}},{"cell_type":"code","metadata":{"id":"ZyAvNCJMmvFF"},"source":["# Import library\n","import os\n","import json\n","import skimage.draw\n","import numpy as np\n","from google.colab.patches import cv2_imshow\n","\n","# Import Mask RCNN\n","from mrcnn import utils\n","from mrcnn import visualize\n","from mrcnn.model import log\n","from mrcnn.config import Config\n","from mrcnn import model as modellib"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vk4gID50K03a"},"source":["## Configurations"]},{"cell_type":"markdown","metadata":{"id":"JgKyUL4pngvE"},"source":["Set model configuration by the ModelConfig class."]},{"cell_type":"code","metadata":{"id":"dq9GY37ml1kr"},"source":["class ModelConfig(Config):\n","    \"\"\"Configuration for training on the oral dataset.\n","    Derives from the base Config class and overrides some values.\n","    \"\"\"\n","    # Give the configuration a recognizable name\n","    NAME = \"OralCancer\"\n","\n","    # Set specific dataset dir\n","    DATASET_DIR = ROOT_DIR + \"DATASET/DATA_220617\"\n","    \n","    # Number of training epochs\n","    EPOCH = 10\n","    \n","    # Number of training steps per epoch\n","    STEPS_PER_EPOCH = 10\n","\n","    # Weight\n","    WEIGHT = \"coco\"\n","    # [Option]: \"coco\", \"imagenet\", \"last\", \"<trained>\"\n","    \n","    # Directory to save logs and model checkpoints\n","    LOGS_DIR = ROOT_DIR + \"LOG\"\n","    \n","    # Batch size\n","    # We use a GPU with 12GB memory, which can fit two images.\n","    # Adjust down if you use a smaller GPU.\n","    IMAGES_PER_GPU = 3\n","\n","    # Number of classes (including background)\n","    NUM_CLASSES = 1 + 3  # Background + number of classes\n","\n","    # Skip detections with < 90% confidence\n","    DETECTION_MIN_CONFIDENCE = 0.9\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uM1thbN-ntjI"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"HUjkwRsOn1O0"},"source":["class OralDataset(utils.Dataset):\n","\n","    def load_dataset(self, dataset_dir, subset):\n","        \"\"\" Load a subset of the oral dataset.\n","        dataset_dir: Root directory of the dataset.\n","        subset: Subset to load: train or val\n","        \"\"\"\n","        # Add classes according to the numbe of classes required to detect\n","        self.add_class(\"OralCancer\", 1, \"G\")\n","        self.add_class(\"OralCancer\", 2, \"Y\")\n","        self.add_class(\"OralCancer\", 3, \"R\")\n","        print(\"\\n[INFO] self.class_info: \\n{}\".format(self.class_info))\n","        \n","        # Train or validation dataset?\n","        assert subset in [\"train\", \"val\", \"test\"]\n","        dataset_dir = os.path.join(dataset_dir, subset)\n","        \n","        # Load annotations\n","        # VGG Image Annotator (up to version 1.6) saves each image in the form:\n","        # { 'filename': '28503151_5b5b7ec140_b.jpg',\n","        #   'regions': {\n","        #       '0': {\n","        #           'region_attributes': {},\n","        #           'shape_attributes': {\n","        #               'all_points_x': [...],\n","        #               'all_points_y': [...],\n","        #               'name': 'polygon'}},\n","        #       ... more regions ...\n","        #   },\n","        #   'size': 100202\n","        # }\n","        # We mostly care about the x and y coordinates of each region\n","        \n","        # Note: In VIA 2.0, regions was changed from a dict to a list.\n","        annotations = json.load(open(os.path.join(dataset_dir, subset+\".json\")))\n","        annotations = list(annotations.values())  # Remove the dict keys\n","        \n","        # The VIA tool saves images in the JSON even if they don't have any annotations.\n","        # Skip unannotated images.\n","        annotations = [a for a in annotations if a['regions']]\n","        \n","        # Add images\n","        for a in annotations:\n","            # Get the x, y coordinaets of points of the polygons that make up\n","            # the outline of each object instance. These are stores in the\n","            # shape_attributes (see json format above)\n","            # The if condition is needed to support VIA versions 1.x and 2.x.\n","            if type(a['regions']) is dict:\n","                polygons = [r['shape_attributes'] for r in a['regions'].values()]\n","            else:\n","                polygons = [r['shape_attributes'] for r in a['regions']]\n","            \n","            # Read labeled class name\n","            label_cls = [r['region_attributes']['OralCancer'] for r in a['regions']]\n","            # Encode class name by class ID\n","            class_ids = {'G': 1,\n","                    'Y': 2,\n","                    'R': 3}\n","            label_ids = [class_ids[a] for a in label_cls]\n","            \n","            # load_mask() needs the image size to convert polygons to masks.\n","            # Unfortunately, VIA doesn't include it in JSON, so we must read\n","            # the image. This is only managable since the dataset is tiny.\n","            image_path = os.path.join(dataset_dir, a['filename'])\n","            image = skimage.io.imread(image_path)\n","            height, width = image.shape[:2]\n","            \n","            self.add_image(\n","                \"OralCancer\",\n","                image_id=a['filename'], # use file name as a unique image id\n","                class_ids=label_ids,\n","                path=image_path,\n","                width=width,height=height,\n","                polygons=polygons\n","            )\n","    \n","    def load_mask(self, image_id):\n","        \"\"\"Generate instance masks for an image.\n","       Returns:\n","        masks: A bool array of shape [height, width, instance count] with\n","            one mask per instance.\n","        class_ids: a 1D array of class IDs of the instance masks.\n","        \"\"\"\n","        # If not a oral dataset image, delegate to parent class.\n","        image_info = self.image_info[image_id]\n","        if image_info[\"source\"] != \"OralCancer\":\n","            return super(self.__class__, self).load_mask(image_id)\n","        \n","        class_ids = image_info[\"class_ids\"]\n","        class_ids = np.array(class_ids, dtype=np.int32)\n","\n","        # Convert polygons to a bitmap mask of shape\n","        # [height, width, instance_count]\n","        info = self.image_info[image_id]\n","        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n","                        dtype=np.uint8)\n","        for i, p in enumerate(info[\"polygons\"]):\n","            # Get indexes of pixels inside the polygon and set them to 1\n","            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n","            mask[rr, cc, i] = 1\n","        \n","        return mask.astype(np.bool_), class_ids\n","\n","    def image_reference(self, image_id):\n","        \"\"\"Return the path of the image.\"\"\"\n","        info = self.image_info[image_id]\n","        if info[\"source\"] == \"OralCancer\":\n","            return info[\"path\"]\n","        else:\n","            super(self.__class__, self).image_reference(image_id)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7d3KxiHO_0gb"},"source":["def train(model):\n","    \"\"\"Train the model.\"\"\"\n","    # Training dataset.\n","    dataset_train = OralDataset()\n","    dataset_train.load_dataset(ROOT_DIR+config.DATASET_DIR, \"train\")\n","    dataset_train.prepare()\n","\n","    # Validation dataset\n","    dataset_val = OralDataset()\n","    dataset_val.load_dataset(ROOT_DIR+config.DATASET_DIR, \"val\")\n","    dataset_val.prepare()\n","\n","    # *** This training schedule is an example. Update to your needs ***\n","    # Since we're using a very small dataset, and starting from\n","    # COCO trained weights, we don't need to train too long. Also,\n","    # no need to train all layers, just the heads should do it.\n","    print(\"Training network heads\")\n","    model.train(dataset_train, dataset_val,\n","          learning_rate=config.LEARNING_RATE,\n","          epochs=config.EPOCH,\n","          layers='heads'\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b2bjrfb2LDeo"},"source":["## Train on the dataset"]},{"cell_type":"code","metadata":{"id":"4Qg7zSVOulkb","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1655583110311,"user_tz":-480,"elapsed":76513,"user":{"displayName":"林永隆","userId":"15129329272009530424"}},"outputId":"b74b4584-aa95-4777-ede2-9a157a36d688"},"source":["if __name__ == '__main__':\n","    \n","    # Configurations\n","    config = ModelConfig()\n","    config.display()\n","\n","    # Create model\n","    model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=config.LOGS_DIR)\n","\n","    # Select weights file to load\n","    if config.WEIGHT.lower() == \"coco\":\n","        weights_path = \"mask_rcnn_coco.h5\" #COCO_WEIGHTS_PATH\n","        # Download weights file\n","        if not os.path.exists(weights_path):\n","            utils.download_trained_weights(weights_path)\n","    elif config.WEIGHT.lower() == \"imagenet\":\n","        # Start from ImageNet trained weights\n","        weights_path = model.get_imagenet_weights()\n","    elif config.WEIGHT.lower() == \"last\":\n","        # Find last trained weights\n","        weights_path = model.find_last()\n","    else:\n","        weights_path = config.WEIGHT\n","    \n","    # Load weights\n","    print(\"Loading weights \", weights_path)\n","    if config.WEIGHT.lower() == \"coco\":\n","        # Exclude the last layers because they require a matching\n","        # number of classes\n","        model.load_weights(weights_path, by_name=True, exclude=[\n","                  \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n","                  \"mrcnn_bbox\", \"mrcnn_mask\"])\n","    else:\n","      model.load_weights(weights_path, by_name=True)\n","    \n","    # Train model\n","    train(model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Configurations:\n","BACKBONE                       resnet101\n","BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n","BATCH_SIZE                     3\n","BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n","COMPUTE_BACKBONE_SHAPE         None\n","DATASET_DIR                    DATASET/DATA_220617\n","DETECTION_MAX_INSTANCES        100\n","DETECTION_MIN_CONFIDENCE       0.9\n","DETECTION_NMS_THRESHOLD        0.3\n","EPOCH                          10\n","FPN_CLASSIF_FC_LAYERS_SIZE     1024\n","GPU_COUNT                      1\n","GRADIENT_CLIP_NORM             5.0\n","IMAGES_PER_GPU                 3\n","IMAGE_CHANNEL_COUNT            3\n","IMAGE_MAX_DIM                  1024\n","IMAGE_META_SIZE                16\n","IMAGE_MIN_DIM                  800\n","IMAGE_MIN_SCALE                0\n","IMAGE_RESIZE_MODE              square\n","IMAGE_SHAPE                    [1024 1024    3]\n","LEARNING_MOMENTUM              0.9\n","LEARNING_RATE                  0.001\n","LOGS_DIR                       LOG\n","LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n","MASK_POOL_SIZE                 14\n","MASK_SHAPE                     [28, 28]\n","MAX_GT_INSTANCES               100\n","MEAN_PIXEL                     [123.7 116.8 103.9]\n","MINI_MASK_SHAPE                (56, 56)\n","NAME                           OralCancer\n","NUM_CLASSES                    4\n","POOL_SIZE                      7\n","POST_NMS_ROIS_INFERENCE        1000\n","POST_NMS_ROIS_TRAINING         2000\n","PRE_NMS_LIMIT                  6000\n","ROI_POSITIVE_RATIO             0.33\n","RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n","RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n","RPN_ANCHOR_STRIDE              1\n","RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n","RPN_NMS_THRESHOLD              0.7\n","RPN_TRAIN_ANCHORS_PER_IMAGE    256\n","STEPS_PER_EPOCH                10\n","TOP_DOWN_PYRAMID_SIZE          256\n","TRAIN_BN                       False\n","TRAIN_ROIS_PER_IMAGE           200\n","USE_MINI_MASK                  False\n","USE_RPN_ROIS                   True\n","VALIDATION_STEPS               50\n","WEIGHT                         coco\n","WEIGHT_DECAY                   0.0001\n","\n","\n","Downloading pretrained model to mask_rcnn_coco.h5 ...\n","... done downloading pretrained model!\n","Loading weights  mask_rcnn_coco.h5\n","\n","[INFO] self.class_info: \n","[{'source': '', 'id': 0, 'name': 'BG'}, {'source': 'OralCancer', 'id': 1, 'name': 'G'}, {'source': 'OralCancer', 'id': 2, 'name': 'Y'}, {'source': 'OralCancer', 'id': 3, 'name': 'R'}]\n","\n","[INFO] self.class_info: \n","[{'source': '', 'id': 0, 'name': 'BG'}, {'source': 'OralCancer', 'id': 1, 'name': 'G'}, {'source': 'OralCancer', 'id': 2, 'name': 'Y'}, {'source': 'OralCancer', 'id': 3, 'name': 'R'}]\n","Training network heads\n","\n","Starting at epoch 0. LR=0.001\n","\n","Checkpoint Path: LOG/oralcancer20220618T2010/mask_rcnn_oralcancer_{epoch:04d}.h5\n","Selecting layers to train\n","fpn_c5p5               (Conv2D)\n","fpn_c4p4               (Conv2D)\n","fpn_c3p3               (Conv2D)\n","fpn_c2p2               (Conv2D)\n","fpn_p5                 (Conv2D)\n","fpn_p2                 (Conv2D)\n","fpn_p3                 (Conv2D)\n","fpn_p4                 (Conv2D)\n","rpn_model              (Functional)\n","mrcnn_mask_conv1       (TimeDistributed)\n","mrcnn_mask_bn1         (TimeDistributed)\n","mrcnn_mask_conv2       (TimeDistributed)\n","mrcnn_mask_bn2         (TimeDistributed)\n","mrcnn_class_conv1      (TimeDistributed)\n","mrcnn_class_bn1        (TimeDistributed)\n","mrcnn_mask_conv3       (TimeDistributed)\n","mrcnn_mask_bn3         (TimeDistributed)\n","mrcnn_class_conv2      (TimeDistributed)\n","mrcnn_class_bn2        (TimeDistributed)\n","mrcnn_mask_conv4       (TimeDistributed)\n","mrcnn_mask_bn4         (TimeDistributed)\n","mrcnn_bbox_fc          (TimeDistributed)\n","mrcnn_mask_deconv      (TimeDistributed)\n","mrcnn_class_logits     (TimeDistributed)\n","mrcnn_mask             (TimeDistributed)\n","Epoch 1/10\n"]},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-1256d19624cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-9d0cc1f8f626>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     19\u001b[0m           \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m           \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'heads'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     )\n","\u001b[0;32m/content/gdrive/MyDrive/MaskRCNN_Tutorial/mrcnn/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001b[0m\n\u001b[1;32m   2374\u001b[0m             \u001b[0;31m# ============================================== change\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2375\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2376\u001b[0;31m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2377\u001b[0m             \u001b[0;31m# workers=workers,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2378\u001b[0m             \u001b[0;31m# use_multiprocessing=workers > 1,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m   def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_generator_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m   def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_generator_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_generator_v1.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator)\u001b[0m\n\u001b[1;32m    344\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    783\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(uid, i)\u001b[0m\n\u001b[1;32m    564\u001b[0m       \u001b[0mThe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mat\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mi\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m   \"\"\"\n\u001b[0;32m--> 566\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gdrive/MyDrive/MaskRCNN_Tutorial/mrcnn/model.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m   1719\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_class_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_masks\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m                 load_image_gt(self.dataset, self.config, image_id,\n\u001b[0;32m-> 1721\u001b[0;31m                               augmentation=self.augmentation)\n\u001b[0m\u001b[1;32m   1722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m             \u001b[0;31m# Skip images that have no instances. This can happen in cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gdrive/MyDrive/MaskRCNN_Tutorial/mrcnn/model.py\u001b[0m in \u001b[0;36mload_image_gt\u001b[0;34m(dataset, config, image_id, augmentation)\u001b[0m\n\u001b[1;32m   1233\u001b[0m     \u001b[0;31m# Load image and mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m     \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m     \u001b[0moriginal_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m     image, window, scale, padding, crop = utils.resize_image(\n","\u001b[0;32m<ipython-input-4-22c535aafe26>\u001b[0m in \u001b[0;36mload_mask\u001b[0;34m(self, image_id)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;31m# Get indexes of pixels inside the polygon and set them to 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mrr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolygon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all_points_y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all_points_x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 280 is out of bounds for axis 1 with size 280"]}]},{"cell_type":"markdown","source":["## Exhibit training curves in tensorboard:"],"metadata":{"id":"Uzyh5l8aL-B_"}},{"cell_type":"code","metadata":{"id":"hBXeH8UXFcqU"},"source":["%load_ext tensorboard\n","%tensorboard --logdir config.LOG_DIR"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","from sklearn.metrics import confusion_matrix\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import skimage.io\n","\n","MODEL_PATH = os.path.join(config.LOG_DIR, \"./oral20220624T1332/mask_rcnn_OralCancer_0036.h5\") #change\n","\n","#config.OUTPUT_DIR = \"./OUTPUT/oral20220624T1332_epoch0036/\"\n","os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n","\n","# Override the training configurations with a few\n","# changes for inferencing.\n","class InferenceConfig(config.__class__):\n","    # Run detection on one image at a time\n","    #GPU_COUNT = 1\n","    IMAGES_PER_GPU = 1\n","config = InferenceConfig()\n","config.display()\n","\n","# Inspect the model in training or inference modes\n","# values: 'inference' or 'training'\n","TEST_MODE = \"inference\"\n","\n","def get_ax(rows=1, cols=1, size=8):\n","    #Return a Matplotlib Axes array to be used in\n","    #all visualizations in the notebook. Provide a\n","    #central point to control graph sizes.\n","    #Adjust the size attribute to control how big to render images\n","    \n","    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n","    return ax\n","\n","# Test dataset\n","dataset_test = OralDataset()\n","dataset_test.load_dataset(config.DATASET_DIR, \"test\")\n","dataset_test.prepare()\n","\n","model = modellib.MaskRCNN(mode=\"inference\", config=config, model_dir=config.LOG_DIR)\n","\n","# Set weights file path\n","if MODEL_PATH is not None:\n","    weights_path = MODEL_PATH\n","else:\n","    weights_path = model.find_last()\n","\n","# Load weights\n","print(\"Loading weights \", weights_path)\n","model.load_weights(weights_path, by_name=True)\n","\n","# ## Run Detection\n","\n","# Initial ground-truth and predictions lists\n","gt_tot_per_image = np.array([])\n","pred_tot_per_image = np.array([])\n","gt_tot = np.array([])\n","pred_tot = np.array([])\n","\n","# Initial mAP list\n","mAP_ = []\n","mprecision_ = []\n","mrecall_ = []\n","mdice = []\n","class_names = []\n","class_names_index = []\n","APs = []\n","count1 = 0\n","\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":192},"id":"47Uv-9tt-DmS","executionInfo":{"status":"ok","timestamp":1655626842299,"user_tz":-480,"elapsed":9,"user":{"displayName":"林永隆","userId":"15129329272009530424"}},"outputId":"917d1561-36db-46b2-e69e-1d6185299b58"},"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nfrom sklearn.metrics import confusion_matrix\\nimport matplotlib\\nimport matplotlib.pyplot as plt\\nimport matplotlib.patches as patches\\nimport skimage.io\\n\\nMODEL_PATH = os.path.join(config.LOG_DIR, \"./oral20220624T1332/mask_rcnn_OralCancer_0036.h5\") #change\\nOUTPUT_DIR = \"./RESULT/oral20220419T0338_epoch0036/\"\\nos.makedirs(OUTPUT_DIR, exist_ok=True)\\n\\n# Override the training configurations with a few\\n# changes for inferencing.\\nclass InferenceConfig(config.__class__):\\n    # Run detection on one image at a time\\n    GPU_COUNT = 1\\n    IMAGES_PER_GPU = 1\\nconfig = InferenceConfig()\\nconfig.display()\\n\\n# Inspect the model in training or inference modes\\n# values: \\'inference\\' or \\'training\\'\\n# TODO: code for \\'training\\' test mode not ready yet\\nTEST_MODE = \"inference\"\\n\\ndef get_ax(rows=1, cols=1, size=8):\\n    #Return a Matplotlib Axes array to be used in\\n    #all visualizations in the notebook. Provide a\\n    #central point to control graph sizes.\\n    #Adjust the size attribute to control how big to render images\\n    \\n    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\\n    return ax\\n\\n# Test dataset\\ndataset_test = OralDataset()\\ndataset_test.load_dataset(config.DATASET_DIR, \"test\")\\ndataset_test.prepare()\\n\\nmodel = modellib.MaskRCNN(mode=\"inference\", config=config, model_dir=config.LOG_DIR)\\n\\n# Set weights file path\\nif MODEL_PATH is not None:\\n    weights_path = MODEL_PATH\\nelse:\\n    weights_path = model.find_last()\\n\\n# Load weights\\nprint(\"Loading weights \", weights_path)\\nmodel.load_weights(weights_path, by_name=True)\\n\\n# ## Run Detection\\n# Ground-truth and predictions lists\\ngt_tot_per_image = np.array([])\\npred_tot_per_image = np.array([])\\ngt_tot = np.array([])\\npred_tot = np.array([])\\n# mAP list\\nmAP_ = []\\nmprecision_ = []\\nmrecall_ = []\\nmdice = []\\nclass_names = []\\nclass_names_index = []\\n\\nAPs = []\\ncount1 = 0\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["\"\"\"\n","# image_id = random.choice(dataset.image_ids, 32) # Random choose several testing samples\n","for image_id in dataset.image_ids:\n","    image, image_meta, gt_class_id, gt_bbox, gt_mask = modellib.load_image_gt(dataset, config, image_id) #change, use_mini_mask=False\n","    info = dataset.image_info[image_id]\n","    print(\"\\nimage ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, dataset.image_reference(image_id)))\n","    \n","    # molded_images = np.extend_dims(modellib.mold_image(image, config), 0) #err\n","    molded_image = np.expand_dims(image, 0)\n","    \n","    # Run object detection\n","    # results = model.detect([image], verbose=1)\n","    results = model.detect(molded_image, verbose=0)\n","\n","    # Display results\n","    ax = get_ax() #1\n","    r = results[0]\n","    \n","    # - Concatenate all materials for \"confusion matrix\" - \n","    if count1 == 0:\n","        # Concatenate \"ground truth\" class, bbox, mask\n","        concat_gt_class_id, concat_gt_bbox, concat_gt_mask = gt_class_id, gt_bbox, gt_mask\n","        # Concatenate all model \"detection result\"\n","        concat_r_rois, concat_r_masks, concat_r_class_ids, concat_r_scores = r['rois'], r['masks'], r['class_ids'], r['scores']\n","    else:\n","        # Concatenate \"ground truth\" class, bbox, mask\n","        concat_gt_class_id = np.concatenate((concat_gt_class_id, gt_class_id), axis=0)\n","        concat_gt_bbox = np.concatenate((concat_gt_bbox, gt_bbox), axis=0)\n","        concat_gt_mask = np.concatenate((concat_gt_mask, gt_mask), axis=2)\n","        # Concatenate model \"detection result\"\n","        concat_r_rois = np.concatenate((concat_r_rois, r['rois']), axis=0)\n","        concat_r_masks = np.concatenate((concat_r_masks, r['masks']), axis=2)\n","        concat_r_class_ids = np.concatenate((concat_r_class_ids, r['class_ids']), axis=0)\n","        concat_r_scores = np.concatenate((concat_r_scores, r['scores']), axis=0)\n","    \n","    count1 += 1\n","    \n","    # - compute gt_tot and pred_tot - \n","    gt, pred = utils.gt_pred_lists(gt_class_id, gt_bbox, r['class_ids'], r['rois'])\n","    gt_tot = np.append(gt_tot, gt)\n","    pred_tot = np.append(pred_tot, pred)\n","    \n","    if len(gt_class_id) == 0 and len(r['class_ids']) > 0:\n","        gt_tot_per_image = np.append(gt_tot_per_image, np.array(0))\n","        pred_tot_per_image = np.append(pred_tot_per_image, np.max(r['class_ids']))\n","        cls = np.max(r['class_ids'])\n","        if dataset.class_names[int(cls)] not in class_names:\n","            class_names.append(dataset.class_names[int(cls)])\n","            class_names_index.append(int(cls))\n","            \n","    elif len(gt_class_id) > 0 and len(r['class_ids']) == 0:\n","        gt_tot_per_image = np.append(gt_tot_per_image, np.max(gt_class_id))\n","        pred_tot_per_image = np.append(pred_tot_per_image, np.array(0))\n","        cls = np.max(gt_class_id)\n","        if dataset.class_names[int(cls)] not in class_names:\n","            class_names.append(dataset.class_names[0])\n","            class_names_index.append(0)\n","            \n","    elif len(r['class_ids']) == 0 and len(gt_class_id) == 0:\n","        gt_tot_per_image = np.append(gt_tot_per_image, np.array(0))\n","        pred_tot_per_image = np.append(pred_tot_per_image, np.array(0))\n","        if dataset.class_names[0] not in class_names:\n","            class_names.append(dataset.class_names[0])\n","            class_names_index.append(0)\n","            \n","    else:\n","        gt_tot_per_image = np.append(gt_tot_per_image, np.max(gt_class_id))\n","        pred_tot_per_image = np.append(pred_tot_per_image, np.max(r['class_ids']))\n","        cls = np.max(gt_class_id)\n","        if dataset.class_names[int(cls)] not in class_names:\n","            class_names.append(dataset.class_names[int(cls)])\n","            class_names_index.append(int(cls))\n","        \n","    color_list = []\n","    cls_index = []\n","    \n","    for m, cls_name in enumerate(r['class_ids']):\n","        # if cls_name == 3:\n","            # color_list.append(colors[2])\n","        # if cls_name == 2:\n","            # color_list.append(colors[1])\n","        # if cls_name == 1:\n","            # color_list.append(colors[0])\n","        cls_index.append(cls_name)\n","    \n","    dice = []\n","    for j in range(len(gt_class_id)):\n","        gt = gt_class_id[j].astype(np.int32) * np.array(gt_mask.astype(np.int32)[:,:,j])\n","        for k in range(len(cls_index)):\n","            seg = cls_index[k].astype(np.int32) * r[\"masks\"].astype(np.int32)[:,:,k]\n","            score = np.sum(seg[gt==gt_class_id[j].astype(np.int32)])*2.0 / (np.sum(seg) + np.sum(gt))\n","\n","            if score <= 1:\n","                dice.append(score)\n","    \n","    if np.mean(dice) <= 1:\n","        print('Dice similarity score is {}'.format(np.mean(dice)))\n","        mdice.append(np.mean(dice))\n","    \n","    image_display = visualize.display_instances(config.OUTPUT_DIR, info[\"id\"], image, r['rois'], r['masks'], r['class_ids'], dataset.class_names, r['scores'], ax=ax) #, title=\"Predictions\"\n","    plt.imshow(image_display)\n","    plt.savefig(config.OUTPUT_DIR+str(info[\"id\"]))\n","    plt.close()\n","    \n","    AP, precisions, recalls, overlaps = utils.compute_ap(gt_bbox, gt_class_id, gt_mask, r['rois'], r['class_ids'], r['scores'], r['masks'])\n","    APs.append(AP)\n","    \n","    # visualize.plot_precision_recall(config.OUTPUT_DIR, info[\"id\"][:-4], AP, precisions, recalls)\n","    # visualize.plot_overlaps(config.OUTPUT_DIR, info[\"id\"][:-4], gt_class_id, r['class_ids'], r['scores'], overlaps, dataset.class_names)\n","\n","    log(\"gt_class_id\", gt_class_id)\n","    log(\"gt_bbox\", gt_bbox)\n","    log(\"gt_mask\", gt_mask)\n","\"\"\""],"metadata":{"id":"2ygoNYHsDTnN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","# ### Precision-Recall\n","\n","print(\"mAP @ IoU=50: \", np.mean(APs))\n","print(\"mean Dice: \", np.mean(mdice))\n","\n","# [change] Confusion matrix ===================================\n","y_test = np.array(gt_tot_per_image)\n","predictions = np.array(pred_tot_per_image)\n","\n","confm = confusion_matrix(y_test, predictions)\n","df_cm = DataFrame(confm, index=[dataset.class_names], columns=[dataset.class_names])\n","\n","sn.set(font_scale=1.4) # for label size\n","sn.heatmap(df_cm, annot=True, cmap='Blues', fmt='g') # font size\n","\n","plt.title('Confusion matrix (per image)')\n","plt.savefig(config.OUTPUT_DIR+\"Confusion_matrix.png\")\n","#plt.show()\n","plt.close()\n","\n","# [change] F1 scores ==========================================\n","fig, ax = plt.subplots(1, 1, figsize=(9, 6), tight_layout=True)\n","    \n","colorlist = colorlist = ['Gray', 'Green', 'Yellow', 'Red', 'Blue', 'Green', 'Yellow', 'Red', 'Blue'] #config.colorlist\n","\n","F1_scores = []\n","\n","for class_id in range(1, len(dataset.class_names)):\n","    mAP, precisions, recalls, _ =\\\n","            utils.compute_ap_pre_class(concat_gt_bbox, concat_gt_class_id, concat_gt_mask, concat_r_rois, concat_r_class_ids, concat_r_scores, concat_r_masks, class_id)\n","    mAR, _ = utils.compute_recall(concat_r_rois, concat_gt_bbox, iou=0.5) \n","    \n","    F1_scores = (2* (np.mean(precisions) * np.mean(recalls)))/(np.mean(precisions) + np.mean(recalls))\n","    \n","    ax.plot(recalls, precisions, color = colorlist[class_id], label=dataset.class_names[class_id] + ' mAP=%.2f, F1 score=%.2f' % (mAP, F1_scores))\n","\n","ax.set_xlabel('Recall (Sensitivity, True positive rate)')\n","ax.set_ylabel('Precision (Positive predicted values)')\n","ax.set_xlim(0, 1)\n","ax.set_ylim(0, 1)\n","\n","ax.legend(loc=\"upper right\", prop={'size': 12})\n","\n","# [change] PR curve ===========================================\n","mmAP, mprecisions, mrecalls, overlaps =\\\n","            utils.compute_ap(concat_gt_bbox, concat_gt_class_id, concat_gt_mask,\n","                     concat_r_rois, concat_r_class_ids, concat_r_scores, concat_r_masks)\n","mmAR, _ = utils.compute_recall(concat_r_rois, concat_gt_bbox, iou=0.5) \n","F1_scores = (2* (np.mean(mprecisions) * np.mean(mrecalls)))/(np.mean(mprecisions) + np.mean(mrecalls))\n","ax.plot(mrecalls, mprecisions, color = 'Blue', label='Total mAP=%.2f, F1 score=%.2f, Mean dice similarity score=%.2f' % (mmAP, F1_scores, mdice))\n","\n","plt.legend()\n","plt.savefig(config.OUTPUT_DIR+\"PR_curve.png\")\n","plt.close()\n","# =============================================================\n","\n","\"\"\""],"metadata":{"id":"jzTMT1Q0DYir"},"execution_count":null,"outputs":[]}]}